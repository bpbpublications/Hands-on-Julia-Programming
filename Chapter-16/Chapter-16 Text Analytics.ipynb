{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Text Analytics\n",
    "\n",
    "This notebook contains the sample source code explained in the book *Hands-On Julia Programming, Sambit Kumar Dash, 2021, bpb Publications. All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/work/books/HOJP/Chapter-16/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Pipeline\n",
    "\n",
    "The text processing can be broadly outlined by the diagram below.\n",
    "\n",
    "![Text Pipeline](414_16_01.png)\n",
    "\n",
    "\n",
    "## Preprocessing \n",
    "\n",
    "Acquisition, tokenization and representation tasks can be loosely considered as preprocessing. \n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units like paragraphs, sentences, words, subwords or characters. Subwords and characters are particularly useful for handling unknown words. However, words are the most commonly used tokens in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{SubString{String}}:\n",
       " \"This is a multi-line text.\"\n",
       " \"And the sentence splitter\"\n",
       " \"may not do a great job on this.\"\n",
       " \"But if no line ending\"\n",
       " \"is provided it works like a charm.\"\n",
       " \"Even keeps Dr. Smith \"\n",
       " \"intact.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using WordTokenizers\n",
    "s1 = \"\"\"This is a multi-line text. And the sentence splitter\n",
    "may not do a great job on this. But if no line ending\n",
    "is provided it works like a charm. Even keeps Dr. Smith \n",
    "intact.\"\"\"\n",
    "split_sentences(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia has a rules based sentence tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{SubString{String}}:\n",
       " \"This is a multi-line text.\"\n",
       " \"And the sentence splitter provided works like a charm.\"\n",
       " \"Even keeps Dr. Smith intact.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = \"This is a multi-line text. And the sentence splitter \"*\n",
    "\"provided works like a charm. Even keeps Dr. Smith \"*\n",
    "\"intact.\"\n",
    "split_sentences(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the tokenizer heuristics by choosing the tokenizer that best meets to your purpose. You may be able to write your own tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poorman: \tSubString{String}[\"I\", \"wont\", \"be\", \"able\", \"to\", \"attend\", \"the\", \"meeting\", \"This\", \"isnt\", \"possible\"]\n",
      "NLTK: \t[\"I\", \"wo\", \"n't\", \"be\", \"able\", \"to\", \"attend\", \"the\", \"meeting.\", \"This\", \"is\", \"n't\", \"possible\", \".\"]\n"
     ]
    }
   ],
   "source": [
    "s3 = \"I won't be able to attend the meeting. This isn't possible.\"\n",
    "set_tokenizer(poormans_tokenize);\n",
    "println(\"Poorman: \\t\", tokenize(s3))\n",
    "set_tokenizer(nltk_word_tokenize);\n",
    "println(\"NLTK: \\t\", tokenize(s3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Relevance\n",
    "\n",
    "Sentences with similar meanings may have different word compositions. There are suffixes or prefixes associated with words. Stemming strips off some of these so that sentences can be compared. Similarly, commonly used words like articles or prepositions may be contribute significantly to the meaning of a sentence. They can be removed as stop words. Capitalization of the words may not also add signficant value to a sentence. Once those are removed, the rest of the words can be compared to check similar sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "educ add valu person\n",
      "educ valu person\n"
     ]
    }
   ],
   "source": [
    "using TextAnalysis\n",
    "\n",
    "doc1 = StringDocument(\"Educate to add value to a person\")\n",
    "prepare!(doc1, strip_case | strip_stopwords | stem_words)\n",
    "println(doc1.text)\n",
    "\n",
    "doc2 = StringDocument(\"Education is valued for a person\")\n",
    "prepare!(doc2, strip_case | strip_stopwords | stem_words)\n",
    "println(doc2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is another way to compare two sentences and find similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"educ add valu person\", \"educ valu person\", \"educ valuabl asset human\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 1.0       0.462709  0.0\n",
       " 0.462709  1.0       0.0\n",
       " 0.0       0.0       1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "doc1 = StringDocument(\"Educate to add value to a person\")\n",
    "doc2 = StringDocument(\"Education is valued for a person\")\n",
    "doc3 = StringDocument(\"Education is valuable asset for a human\")\n",
    "c = Corpus([doc1, doc2, doc3])\n",
    "prepare!(c, strip_case | strip_stopwords | stem_words)\n",
    "update_lexicon!(c)\n",
    "\n",
    "println((c[1].text, c[2].text, c[3].text))\n",
    "tfm =  tf_idf(dtm(c))\n",
    "cs = Matrix(tfm*tfm')\n",
    "d = sqrt.(diag(cs))\n",
    "cs ./ (d*d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Embeddings are pretty much the de-facto standards for representations in neural computing today. Here a word or token is converted to a vector of numbers that captures the word and its interaction with other words and the context in which it is used. Word2Vec, GloVe and Transformer based models are used to generate embeddings.\n",
    "\n",
    "\n",
    "<i><b> Note: </b>If you are running this code for the first time, it may download 1.5 GBs of data over the internet and may take hours to complete depending on your network speed and connectivity. Ensure, you have a good data connectivity during this period. Once downloaded, the embeddings shall be cached for subsequent use. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n",
    "using Embeddings\n",
    "const et = load_embeddings(Word2Vec)\n",
    "const w2idx = Dict((w => i for (i, w) in enumerate(et.vocab)))\n",
    "embedding(w) = et.embeddings[:, w2idx[w]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`embedding(\"queen\") = embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")`\n",
    "\n",
    "While the above rule seems intiutive, the cosine distance measures may not be as accurate. \"king\" is closer to the computation while \"queen\" is the second best choice. Hence, it is important that you collect several of the neighborhood words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element view(::Vector{Int64}, 1:5) with eltype Int64:\n",
       "  6031\n",
       "  9970\n",
       " 21459\n",
       " 16561\n",
       " 14961"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
    "cosdist = vec(sum(res .* et.embeddings, dims=1))\n",
    "ids = partialsortperm(cosdist, 1:5, rev=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Tuple{String, Float32}}:\n",
       " (\"king\", 0.8990531)\n",
       " (\"queen\", 0.80069506)\n",
       " (\"monarch\", 0.69625)\n",
       " (\"princess\", 0.6639391)\n",
       " (\"prince\", 0.6048718)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(et.vocab[ii], cosdist[ii]) for ii in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis\n",
    "\n",
    "These techniques are often used in topic modeling. One can intuitively realize the first two sentences are similar while the last one is slightly different. The same can be seen from the results obtained from using the methods `lsa` and `lda`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"educ add valu person\", \"educ valu person\", \"educ valuabl asset human\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVD{Float64, Float64, Matrix{Float64}}\n",
       "U factor:\n",
       "3×3 Matrix{Float64}:\n",
       " 0.0  -0.931471  -0.363815\n",
       " 0.0  -0.363815   0.931471\n",
       " 1.0   0.0        0.0\n",
       "singular values:\n",
       "3-element Vector{Float64}:\n",
       " 0.47571307544817304\n",
       " 0.3266291290597002\n",
       " 0.16072253690428198\n",
       "Vt factor:\n",
       "3×7 Matrix{Float64}:\n",
       " -3.63248e-17  0.57735  0.0  0.57735   4.92112e-17   4.92112e-17  0.57735\n",
       " -0.783248     0.0      0.0  0.0      -0.439615     -0.439615     0.0\n",
       " -0.62171      0.0      0.0  0.0       0.55384       0.55384      0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "doc1 = StringDocument(\"Educate to add value to a person\")\n",
    "doc2 = StringDocument(\"Education is valued for a person\")\n",
    "doc3 = StringDocument(\"Education is valuable asset for a human\")\n",
    "c = Corpus([doc1, doc2, doc3])\n",
    "prepare!(c, strip_case | strip_stopwords | stem_words)\n",
    "update_lexicon!(c)\n",
    "\n",
    "println((c[1].text, c[2].text, c[3].text))\n",
    "lsa(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " 0.142857   ⋅    0.428571   ⋅    0.285714  0.142857   ⋅ \n",
       "  ⋅        0.25   ⋅        0.25   ⋅        0.25      0.25, [0.75 1.0 0.25; 0.25 0.0 0.75])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = DocumentTermMatrix(c)\n",
    "lda(m, 2, 2000, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classifiers\n",
    "\n",
    "Naive Bayes classification is a simplified Bayes rules based classifier. The words are assumed to be independent and dependent on the class to which the sentence belongs to. Even with just using two sentences for training, the predicted sentence is given significant weightage for `:g1`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol, Float64} with 2 entries:\n",
       "  :g2 => 0.278623\n",
       "  :g1 => 0.721377"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TextAnalysis\n",
    "m = NaiveBayesClassifier([:g1, :g2])\n",
    "fit!(m, \"Education is valued for a person\", :g1)\n",
    "fit!(m, \"Education is valuable asset for a human\", :g2)\n",
    "predict(m, \"Education is valued for a man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summarization\n",
    "\n",
    "TextRank algorithm can be used to identify the sentences that provide the most significant information for a paragraph. They can be given higher rank and can be used to summarize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Neural computing has made significant inroads into most text processing tasks.The traditional statistical approaches though relevant are being experimented alongside deep learning techniques.The codes or texts described here are for the ease of learning and understanding and should not be considered for a production performance.However, it is an over simplification from your real life use cases and there may be certain tasks that are overlapping.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TextAnalysis\n",
    "s = \"Research in the text analysis space has been so rapid that \"*\n",
    "\"many concepts that have been relevant a few years ago are \"*\n",
    "\"getting continuously augmented with newer concepts and \"*\n",
    "\"knowledge. Neural computing has made significant inroads \"*\n",
    "\"into most text processing tasks. The traditional statistical \"*\n",
    "\"approaches though relevant are being experimented alongside \"*\n",
    "\"deep learning techniques. We shall discuss a few commonly used \"*\n",
    "\"packages for text analytics and suggest the reader to explore \"*\n",
    "\"and learn newer techniques as they proceed with expanding their \"*\n",
    "\"horizons. The codes or texts described here are for the ease of \"*\n",
    "\"learning and understanding and should not be considered for a \"*\n",
    "\"production performance. Most text processing tasks can be \"*\n",
    "\"considered as a simplified pipeline of activities as shown in \"*\n",
    "\"Figure 16.1. However, it is an over simplification from your \"*\n",
    "\"real life use cases and there may be certain tasks that are \"*\n",
    "\"overlapping.\"\n",
    "\n",
    "summary = *(summarize(StringDocument(s), ns=4)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "The quality of summarization can be compared with a human reference summarization using ROUGE_N scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.369047619047619"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [\"Text analysis research has been rapid. \",\n",
    "       \"Neural computing is being used alongwith statistical techniques. \",\n",
    "       \"Deep learning is gaining popularity as well. \",\n",
    "       \"Code and text provides here are for ease of learning and should not be used for production performance.\"]\n",
    "rouge_n(ref, summary, 2, avg=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9457236842105263"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_n(ref, summary, 1, avg=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "We shall use the pretrained `NERTagger` from the `TextModels` package. The NERs recognized are: Person(PER), Others(O), Locations (LOC), Organizations(ORG), Miscellaneous (MISC). \n",
    "\n",
    "<i><b>Note:</b> This part of the code is temporarily commented out as TextModels package is not compatible with Julia 1.6 release. We will update it to the latest version as soon as the package is supported. However, you can run this code in a Julia 1.5.x installation. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "using Pkg\n",
    "Pkg\"add TextModels\"\n",
    "\n",
    "using TextModels # This package is currently not compatible with Julia 1.6. You can run this code with Julia 1.5. \n",
    "\n",
    "ner = NERTagger()\n",
    "s1 = \"Kumar is from Bangalore and works for Julia Computing.\"\n",
    "println(ner(s1))\n",
    "s2 = \"Julia language is easy to learn.\"\n",
    "println(ner(s2))\n",
    "\n",
    "=#\n",
    "\n",
    "#= Output\n",
    "[\"PER\", \"O\", \"O\", \"LOC\", \"O\", \"O\", \"O\", \"ORG\", \"ORG\", \"O\"]\n",
    "[\"PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ULMFiT\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "## Exercises"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
